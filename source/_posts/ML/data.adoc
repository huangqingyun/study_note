---
title: 数据相关阶段
date: 2022-09-14 16:17:49
tags: ["python","pytorch","data","ML"]
---
= 数据集
青云 黄 <qyhuang@jiliason.com>

:imagesdir: ./img/

数据集：是可以直接进行训练的数据，即采集的已经经过标注、清洗、整理、转换、特征提取后的数据。

== 深度学习模型任务一般流程

image::机器学习简要流程.png[]

=== 深度学习模型任务时间占比

2020年Anaconda公司的调查:

image::机器学习任务时间占比.png[]

== 数据获取

* 数据集下载：
** https://zhuanlan.zhihu.com/p/25138563[各领域公开数据集下载]
* 爬数据
* 自己采集
* 生成数据；例如，GAN生成图片，PS图片，例如目标检测，可以将目标物体放到不同背景生成图片数据
* 数据增强，
** 图片：进行各种滤镜或位置变化，拉伸，通道提取，加噪音等生成新的数据
** 文本：对其进行不同语音翻译，后再将翻译结果再翻译回原来的语言来，这样就可以得到多个语义接近的不同句子。



数据融合：把不同数据源数据整理成一个一致性的数据集。
不同数据库和不同表进行数据整合，合并。

====
* 找合适数据
* 原始数据：需要清理，整理
* 数据增强/扩充：再数据不足时使用，再现有数据基础上进行增强，扩充
* 数据合成/生成
====

== 数据标注

人工标注

* 专家标注
* 众包
** 标注任务设计要简单，容易操作；不要太复杂。
** 标注成本
** 质量控制

自训练：适用于部分数据标注的

少量标注的数据 --训练--> 模型 --预测未标注数据--> 预测标注数据？-----合并到标注数据---训练---> 模型

主动学习：适用于部分数据标注的，和自训练不同的是通过人工介入参与发挥人的作用，就是把数据中**重要的特征标签**进行人工标注

主动学习+自训练： 

少量标注的数据 --训练--> 模型 --预测未标注数据--> 预测标注数据中最置信的数据-----合并到标注数据---训练---> 模型
少量标注的数据 --训练--> 模型 --预测未标注数据--> 预测标注数据中不置信的数据--人工标注----合并到标注数据---训练---> 模型

弱监督学习：半自动生成标注，但比人工标注要差

数据编程，找规律进行标注，比如客服话术规律，对关键词特征进行判断是否标注为客服

== 探索性数据分析

进行探索性数据分析（EDA）是为了获得对数据的初步了解。在一个典型的数据科学项目中，会做的第一件事就是通过执行EDA来 "盯住数据"，以便更好地了解数据。

通常使用的三大EDA方法包括：

* 描述性统计：平均数、中位数、模式、标准差。
* 数据可视化：热力图（辨别特征内部相关性）、箱形图（可视化群体差异）、散点图（可视化特征之间的相关性）、主成分分析（可视化数据集中呈现的聚类分布）等。
* 数据整形：对数据进行透视、分组、过滤等。

过滤掉无用的及少数据的**特征数据/列**,例如保留特征列中有数据的占比70%的，否则就舍弃该特征列。
检查特征列数据类型是否正确，并转到正确的数据类型
检查特征列数据值是否合理，例如人的年龄有负数，或大于200的等。（均值，最大值，最小值，计数等统计学方法来检查数据噪音）
分析特征列数据相关度（协方差）

== 数据清理

数据清洗 (Data cleaning)– 对数据进行重新审查和校验的过程，目的在于删除重复信息、纠正存在的错误，并提供数据一致性 。 

清洗目的：

* 过滤错误数据
* 数据降噪,即剔除一些无关信息
* 使提供的数据和真实的趋向一致

清洗方向：

* 检查数据合理性：比如爬到的数据是否满足需求；
* 检查数据有效性：数据量是否足够大，以及是否都是相关数据；
* 检查工具:收集工具是否有bug；

因为人为、软件、业务导致的异常数据还是比较多的，比如性别数据的缺失、年龄数据的异常（负数或者超大的数），而大多数模型对数据都有基本要求，比如不能缺失，而异常数据对模型是有影响的，因此通常都需要进行预处理；

* 缺失处理：
** bug导致缺失:因为程序bug导致缺失，这种缺失通常是少数的，一般都需要进行某种方式的填充；
** 正常业务情况导致缺失：比如性别字段本身就是可以不填的，那么性别就存在缺失，且这种缺失可能是大量的，这里就要首先评估该字段的重要性以及缺失率，再考虑是填充，还是丢弃；
* 异常处理：
** 绝对异常:比如人的年龄200岁，这个数据放到什么场景下都是异常；
** 统计异常:比如某个用户一分钟内登陆了100次，虽然每一次登陆看着都是正常的，但是统计起来发现是异常的（可能是脚本在自动操作）；
** 上下文异常:比如冬天的北京，晚上温度为30摄氏度，虽然但看数据是正常，但是跟当前的日期、时间一关联，发现是异常；

是对数据噪音进行降噪、清理得到相对比较干净的数据。

数据错误：收集的数据和真实的存在不一致。
干净的数据集和噪音大的数据集预测精度相差可能1%~2%，实际情况我们无法知道在干净数据集训练的模型或噪音数据集训练的模型预测的精度。

* 离群值，异常值
* 规则，语义，语法等错误
* 模式

== 数据变换

把数据从一种格式转换成机器学习所需要的另一种格式，要注意平衡数据大小，数据质量和数据读取速度。

数值归一化
* 计算速度

Min-max 

图片,音频，视频等媒体文件：统一文件格式，变换尺寸变换，缩放，裁剪等
* 存储大小
* 数据质量
* 读取速度

文本：对于机器学习关键是理解文字，而不是语法的正确性
* 语法化/词根化： am,are,is -> be;car,cars,car's,cars'->car
* 词元化（Tokenization）:机器学习算法里最小的单元

== 数据特征提取

基本步骤：

* 特征构建：
** 特征组合：例如组合日期、时间两个特征，构建是否为上班时间(工作日的工作时间为1，其他为0)特征，特征组合的目的通常是为了获得更具有表达力、信息量的新特征；
** 特征拆分：将业务上复杂的特征拆分开，比如将登陆特征，拆分为多个维度的登陆次数统计特征;拆分为多个的好处一个是从多个维度表达信息，另一个多个特征可以进行更多的组合；
** 外部关联特征:例如通过时间信息关联到天气信息，这种做法是很有意义的，首先天气数据不是原始数据集的，因此这样想当于丰富了原始数据，通常来讲会得到一个比仅仅使用原始数据更好的结果，不仅仅是天气，很多信息都可以这样关联（比如在一个Kaggle上的房屋预测问题上，可以通过年份关联到当时的一些地方政策、国际大事等等，都是有影响的，比如金融危机）；
* 特征选择：
** 特征自身的取值分布:主要通过方差过滤法，比如性别特征，1000个数据，999个是男的，1个是女的，这种特征由于自身过于偏斜，因此是无法对结果起到足够的帮助；
** 特征与目标的相关性：可以通过皮尔逊系数、信息熵增益等来判断，思路是如果一个特征与目标的变化是高度一致的，那么它对于预测目标就是具有很大指导意义的；

int/float: 直接使用或将对数值精度不敏感的值映射到敏感区间(bin to n unique int value):例如房价100万，101万对于的预测房价1万的相差不会太在意,可以将100~110区间的价格映射为一种数据。
one-hot(独热编码):

